{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450a73e0",
   "metadata": {},
   "source": [
    "# Modulus8 Dataset\n",
    "\n",
    "This script generates a synthetic dataset, named \"Modulus8\", designed to challenge multi-class classification models in a high-dimensional feature space. The dataset comprises records with 100 features: 50 of them influence the determination of the target class, while the remaining 50 introduce noise, thereby adding ambiguity to the classification task.\n",
    "\n",
    "The dataset generation commences by creating random integer values between 0 and 100 for both predictor and dummy features. To determine the target class, the values of the initial 50 features are summed, followed by a modulus operation with 8. The outcome of this modulus operation is the class label, which can range from 0 to 7. This methodological twist ensures that classes aren't merely based on distinct value ranges but rather on a summation across multiple features, complicated further by a modulus operation.\n",
    "\n",
    "Every record in the \"Modulus8\" dataset includes a unique ID, the 100 features (50 predictors and 50 noise features), and the target class. The target class, represented as integers between 0 and 7, stems from the modulus operation result. While generating the dataset, 3000 samples are initially created for each class. To achieve the desired class distribution—100 samples for class 0, 200 for class 1, and so forth until 3000 for class 7—the dataset undergoes subsampling.\n",
    "\n",
    "The very architecture of this dataset creates a formidable classification challenge. With the entangled summation and modulus operations and the added noise features, pinpointing class boundaries becomes a non-trivial task for models.\n",
    "\n",
    "The resulting \"Modulus8\" dataset, with its multifaceted design and inherent intricacies, offers a rigorous testing ground for machine learning enthusiasts. Although synthetically devised, it mirrors complexities encountered in real-world scenarios, truly evaluating the proficiency of various classification algorithms.\n",
    "\n",
    "--- \n",
    "\n",
    "This updated description now combines the modulus operation's usage with the method of attaining the desired class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "692de854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46711af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'modulus8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5994fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./../../processed/{dataset_name}/'\n",
    "outp_fname = os.path.join(output_dir, f'{dataset_name}.csv')\n",
    "outp_chart_fname = os.path.join(output_dir, f'{dataset_name}_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6290e1d",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b422210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value=0):\n",
    "    np.random.seed(seed_value)\n",
    "    random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d50a0d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(n_samples: int, n_predictors: int, n_dummy: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Generate random predictor and dummy values for a specified number of samples.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples (int): Number of samples to generate.\n",
    "    - n_predictors (int): Number of predictor features.\n",
    "    - n_dummy (int): Number of dummy/noise features.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Generated predictor values, dummy values, computed class labels.\n",
    "    \"\"\"\n",
    "    predictors = np.random.randint(0, 101, size=(n_samples, n_predictors))\n",
    "    dummy_features = np.random.randint(0, 101, size=(n_samples, n_dummy))\n",
    "    labels = (predictors.sum(axis=1) % 8)\n",
    "    \n",
    "    return predictors, dummy_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54d9ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_class_data(\n",
    "        predictors: np.array,\n",
    "        dummy_features: np.array,\n",
    "        labels: np.array,\n",
    "        class_label: int,\n",
    "        n_samples: int\n",
    "    ) -> tuple:\n",
    "    \"\"\"\n",
    "    Subsample predictor and dummy values to match a specified class label and number of samples.\n",
    "    \n",
    "    Parameters:\n",
    "    - predictors (np.array): Predictor values.\n",
    "    - dummy_features (np.array): Dummy values.\n",
    "    - labels (np.array): Computed class labels.\n",
    "    - class_label (int): Desired class label.\n",
    "    - n_samples (int): Number of samples to subsample.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Subsampled predictor values, dummy values, target values.\n",
    "    \"\"\"\n",
    "    mask = (labels == class_label)\n",
    "    \n",
    "    selected_predictors = predictors[mask][:n_samples]\n",
    "    selected_dummies = dummy_features[mask][:n_samples]\n",
    "    selected_targets = np.array([class_label] * n_samples)\n",
    "    \n",
    "    return selected_predictors, selected_dummies, selected_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2f2215b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(\n",
    "        class_samples: list = [100, 200, 300, 500, 800, 1200, 2000, 3000],\n",
    "        n_predictors: int = 50,\n",
    "        n_dummy: int = 50\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for multiclass classification.\n",
    "    \n",
    "    Parameters:\n",
    "    - class_samples (list): A list containing the number of samples desired for each class.\n",
    "    - n_predictors (int): Number of actual predictor features.\n",
    "    - n_dummy (int): Number of dummy/noise features.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A pandas DataFrame containing the generated dataset.\n",
    "    \"\"\"   \n",
    "    all_predictors, all_dummies, all_targets = [], [], []\n",
    "    \n",
    "    predictors, dummy_features, labels = generate_samples(25000, n_predictors, n_dummy)\n",
    "    \n",
    "    for class_label in range(8):\n",
    "        selected_predictors, selected_dummies, selected_targets = subsample_class_data(predictors, dummy_features, labels, class_label, class_samples[class_label])\n",
    "        \n",
    "        all_predictors.append(selected_predictors)\n",
    "        all_dummies.append(selected_dummies)\n",
    "        all_targets.append(selected_targets)\n",
    "\n",
    "    # Concatenate the results\n",
    "    predictors = np.vstack(all_predictors)\n",
    "    dummy_features = np.vstack(all_dummies)\n",
    "    target = np.concatenate(all_targets)\n",
    "\n",
    "    # Create a dataframe\n",
    "    df = pd.DataFrame(\n",
    "        np.hstack([predictors, dummy_features]),\n",
    "        columns=[f'predictor_{i+1}' for i in range(n_predictors)] + \\\n",
    "        [f'dummy_{i+1}' for i in range(n_dummy)]\n",
    "    )\n",
    "    \n",
    "    # Add the target column\n",
    "    df['target'] = target\n",
    "    \n",
    "    # Shuffle data\n",
    "    df = df.sample(frac=1.0, replace=False)\n",
    "\n",
    "    # Add an ID column\n",
    "    df['id'] = range(len(df))\n",
    "    \n",
    "    # Arrange the columns\n",
    "    df = df[['id'] + [col for col in df if col != 'id']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "461f3dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8100, 102)\n",
      "      id  predictor_1  predictor_2  predictor_3  predictor_4  predictor_5  \\\n",
      "7451   0           92           34           12           95           19   \n",
      "6586   1           23           60           17           90           82   \n",
      "4081   2           46           46           12           48           63   \n",
      "2655   3           81           11           12           63            5   \n",
      "3339   4           47           25           72           91           40   \n",
      "\n",
      "      predictor_6  predictor_7  predictor_8  predictor_9  ...  dummy_42  \\\n",
      "7451            2           55           87            6  ...        81   \n",
      "6586           40           80           82           70  ...        96   \n",
      "4081           71           91           52           61  ...        67   \n",
      "2655           35           92            1           54  ...        37   \n",
      "3339           93           27           71           59  ...        96   \n",
      "\n",
      "      dummy_43  dummy_44  dummy_45  dummy_46  dummy_47  dummy_48  dummy_49  \\\n",
      "7451        84        60        13        43        26        32        54   \n",
      "6586        24        86        28        13         9        76        52   \n",
      "4081        66        42        85        69        34         6        84   \n",
      "2655        95        49        81        36        86        57        29   \n",
      "3339        43        94        67        98        54        79        70   \n",
      "\n",
      "      dummy_50  target  \n",
      "7451        28       7  \n",
      "6586        99       7  \n",
      "4081        94       6  \n",
      "2655        99       5  \n",
      "3339        12       6  \n",
      "\n",
      "[5 rows x 102 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "set_seed()\n",
    "\n",
    "# Generate original dataset\n",
    "data = generate_dataset()\n",
    "\n",
    "print(data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce176f",
   "metadata": {},
   "source": [
    "# Verify data was correctly generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75962f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def verify_target_calculation(df: pd.DataFrame, n_predictors: int = 50) -> bool:\n",
    "    \"\"\"\n",
    "    Verify that the target is correctly calculated based on the predictor features.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The generated dataset.\n",
    "    - n_predictors (int): Number of predictor features.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if all targets are correctly calculated, False otherwise.\n",
    "    \"\"\"\n",
    "    # Calculate the target based on the predictor features\n",
    "    calculated_targets = df.iloc[:, 1:n_predictors+1].sum(axis=1) % 8\n",
    "\n",
    "    # Check if the calculated targets match the actual targets in the dataframe\n",
    "    matches = (calculated_targets == df['target']).all()\n",
    "\n",
    "    # Check the counts per class\n",
    "    class_counts = df['target'].value_counts().sort_index().values\n",
    "    desired_counts = [100, 200, 300, 500, 800, 1200, 2000, 3000]\n",
    "\n",
    "    correct_counts = all(count == desired for count, desired in zip(class_counts, desired_counts))\n",
    "\n",
    "    return matches and correct_counts\n",
    "\n",
    "print(verify_target_calculation(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0d52f",
   "metadata": {},
   "source": [
    "# Save Main Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5ab3560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(outp_fname, index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544354b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
